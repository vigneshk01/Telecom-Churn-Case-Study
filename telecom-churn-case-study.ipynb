{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 1. Import all necessary libraries. </br> &nbsp;&nbsp;&nbsp;&nbsp;LogisticRegression, Decision Tree classifier, Random Forest, RFE and PCA packages etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "%matplotlib inline\n",
    "\n",
    "# Set custom display properties in pandas\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 900) \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fast_ml    ## Required for constant feature identification package\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.decomposition import PCA, IncrementalPCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve, f1_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 2. Initialize the telecom_churn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df = pd.read_csv(\"./telecom_churn_data.csv\")\n",
    "telecom_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <div style=\"color: orange; font-size: 22px;\">1. Function Combine_features: </br> The below function combines all the features that are part of \"Good phase\" by stripping the unique identifiers. </br>It then takes the mean among the same features and finally creating a new derived feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(df, cols, pat1='_6' , pat2='_7' ,to_append='_good_phase'):\n",
    "    drop_lst = []\n",
    "    new_cols = []\n",
    "    month_col = cols[cols.str.contains(pat='jun_|jul_')].str.replace(pat=\"(jun_|jul_)\", repl=\"\", regex=True).unique()\n",
    "    cols = cols.str.replace(pat=\"(_\\d$|jun_|jul_)\", repl=\"\", regex=True).unique()\n",
    "    for col in cols:\n",
    "        if col in month_col:\n",
    "            new_col = col + to_append\n",
    "            col1 = 'jun_' + col\n",
    "            col2 = 'jul_' + col\n",
    "        else:\n",
    "            new_col = col + to_append\n",
    "            col1 = col + pat1\n",
    "            col2 = col + pat2\n",
    "            \n",
    "        df[new_col] = df[[col1, col2]].mean(axis=1)      #############################################  Mean or Median to be decided \n",
    "        drop_lst.extend([col1,col2])\n",
    "        new_cols.extend([new_col])\n",
    "    return drop_lst, new_cols   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <div style=\"color: orange; font-size: 22px;\">2. Function find_outliers </br>Outlier Analysis using Boxplot IQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(df):\n",
    "    temp_df = pd.DataFrame(columns= ['col', 'lower_threshold', 'iqr_q1', 'iqr', 'iqr_q3', 'upper_threshold', 'outliers_cnt'])\n",
    "    num_cols = df.select_dtypes(include=np.number).columns\n",
    "    for col in num_cols:\n",
    "        arr = df[col][df[col].notna()]\n",
    "        iqr_q3 = np.quantile(df[col], 0.75)\n",
    "        iqr_q1 = np.quantile(df[col], 0.25)\n",
    "        \n",
    "        iqr = iqr_q3 - iqr_q1\n",
    "        iqr_upper_threshold = iqr_q3 + (1.5 * iqr)\n",
    "        iqr_lower_threshold = iqr_q1 - (1.5 * iqr)\n",
    "        \n",
    "        outliers = arr[(arr > iqr_upper_threshold) | (arr < iqr_lower_threshold)]\n",
    "        to_add = pd.Series({'col': col, 'lower_threshold': iqr_lower_threshold, 'iqr_q1': iqr_q1, 'iqr': iqr, 'iqr_q3': iqr_q3, 'upper_threshold': iqr_upper_threshold, 'outliers_cnt': len(outliers)})\n",
    "        temp_df = pd.concat([temp_df,to_add.to_frame().T])\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <div style=\"color: orange; font-size: 22px;\">3. Function check_col_null_pct: </br>Check the columns null percentage and return the columns based on the given threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_col_null_pct(df, thresh=0, incl_all=1):\n",
    "    col_null_pct = (df.isna().sum()/len(df))*100\n",
    "    cols_na_abv_thresh = col_null_pct[col_null_pct > thresh]\n",
    "    cols_na_bel_thresh = col_null_pct[col_null_pct < thresh]\n",
    "    return cols_na_abv_thresh.sort_values(ascending=False) , cols_na_bel_thresh.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 3. Check the shape and size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.size\n",
    "telecom_df.shape\n",
    "telecom_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \">Observation 1:</div> The size of the data set is 22599774 and the data set contains 226 columns altogether. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 4. Check for any duplicate entries in the data set. Also check if there is any duplicates in mobile number column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df[telecom_df.duplicated()]\n",
    "telecom_df['mobile_number'].is_unique  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \"> Observation 2:</div> There are no duplicate row entries in the dataset or in mobile number column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 5. Check for column null percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_gt40_na , cols_with_le40_na = check_col_null_pct(telecom_df, 40)\n",
    "len(cols_with_gt40_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \"> Observation 3: <li>There are 40 columns that have null percentage greater than 40%. Since these columns have high percentage of null values, we decide to drop these features.</li></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df = telecom_df[cols_with_le40_na.index.sort_values(ascending=True)]\n",
    "cols_with_le40_na[cols_with_le40_na > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \"> Observation 4:</div> After dropping features that have more than 40% null values. We could see that there are still 126 columns that have some null values. Therefore, we have to impute those missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 6. Check for any constant features or feature that has only one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_ml import feature_selection as fs\n",
    "\n",
    "const_features_df = fs.get_constant_features(telecom_df, threshold=100, dropna=True)\n",
    "const_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \"> Observation 5: \n",
    "<li>The following features all have constant values:\n",
    "<ul>circle_id, last_date_of_month_6, last_date_of_month_7, last_date_of_month_8, last_date_of_month_9, loc_ic_t2o_mou, loc_og_t2o_mou, std_ic_t2o_mou_6, std_ic_t2o_mou_7, std_ic_t2o_mou_8, std_ic_t2o_mou_9, std_og_t2c_mou_6, std_og_t2c_mou_7, std_og_t2c_mou_8, std_og_t2c_mou_9, std_og_t2o_mou</ul>\n",
    "<li> These constant features add little value to the model, hence we drop those features.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df = telecom_df.drop(const_features_df['Var'].to_list(), axis=1)\n",
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\">7. Plot the bar chart for columns with less than 10% of NULL values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,cols_with_le10_na = check_col_null_pct(telecom_df, 10)\n",
    "plt.figure(figsize=(30,2))\n",
    "cols_with_le10_na[(cols_with_le10_na > 0)].plot.bar()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 8. Convert Date column to day in numbers format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df[['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9']] = telecom_df[['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9']].apply(lambda x: pd.to_datetime(x).dt.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 9. Check for any outliers in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_outliers(telecom_df).sort_values(by=['outliers_cnt'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \"> Observation 6: <li>There are outliers in the dataset. But due to high class imbalance, We therefore decide not to cap new outliers as it may have impact on our Model metrics.</li></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 10. Impute Missing Values: <li>Since there are lot of missing values in the dataset, we therefore decide to impute it. </br> <li>Also most of the columns are skewed, hence we use \"median\" as a strategy to impute it. </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_bk = telecom_df.copy()\n",
    "# telecom_df = telecom_df_bk\n",
    "\n",
    "to_impute_df = telecom_df.select_dtypes(include = np.number)\n",
    "\n",
    "si = SimpleImputer(strategy='median')\n",
    "imputed_arr = si.fit_transform(to_impute_df)\n",
    "df_imputed = pd.DataFrame(imputed_arr, columns = to_impute_df.columns)\n",
    "\n",
    "telecom_df = telecom_df[telecom_df.columns.difference(to_impute_df.columns)]\n",
    "telecom_df = pd.concat([telecom_df, df_imputed], axis=1)\n",
    "telecom_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 11. Filter the high value customers who have recharged more than 70% of the average recharge value during the good phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# telecom_df['total_rech_amt_good_phase'] = telecom_df[['total_rech_amt_6','total_rech_amt_7']].sum(axis=1)\n",
    "# telecom_df = telecom_df.drop(['total_rech_amt_6','total_rech_amt_7'], axis=1)\n",
    "# telecom_df = telecom_df[(telecom_df['total_rech_amt_good_phase'] >= telecom_df['total_rech_amt_good_phase'].quantile(0.7))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df['total_rech_amt_good_phase']  = telecom_df[['total_rech_amt_6','total_rech_amt_7']].mean(axis=1)\n",
    "telecom_df = telecom_df.drop(['total_rech_amt_6','total_rech_amt_7'], axis=1)\n",
    "telecom_df = telecom_df[(telecom_df['total_rech_amt_good_phase'] >= telecom_df['total_rech_amt_good_phase'].quantile(0.7))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \"> Observation 6:\n",
    "After filtering the high value customers, we could notice the dataset is now reduced to ~30k rows </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 12. Tag the churned customers (1 or 0) by applying conditions on the following fourth month columns: total_ic_mou_9, total_og_mou_9, vol_2g_mb_9, vol_3g_mb_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df['churn'] = telecom_df.apply(lambda x: 1 if ((x['total_ic_mou_9'] < 1) & (x['total_og_mou_9'] < 1) & (x['vol_2g_mb_9'] < 1 ) & (x['vol_3g_mb_9'] < 1) )  else  0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 13. Rename columns_8 as action phase based on business requirements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df.columns = telecom_df.columns.str.replace(pat='_8',repl='_action_phase')\n",
    "telecom_df = telecom_df.rename(columns={'aug_vbc_3g': 'vbc_3g_action_phase'})\n",
    "telecom_df.filter(like='_8').columns\n",
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 14. Find all columns related to churn phase or cols with _9 in name. and drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_cols_to_drop = telecom_df.filter(like='_9').columns.to_list()\n",
    "sep_cols_to_drop.append('sep_vbc_3g')\n",
    "sep_cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df = telecom_df.drop(sep_cols_to_drop, axis=1)\n",
    "telecom_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \"> Observation 7:\n",
    "After dropping the churn phase columns. We could now notice that the number of columns have been reduced to 131. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 15. Filter features that are identified by months _6 & _7, in order to combine those features as \"good phase\" and also check the datatypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_combine =  telecom_df.filter(regex='.*(jun_|jul_|_6|_7).*',axis=1).columns\n",
    "telecom_df[cols_to_combine].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <div style=\"color: lightblue; font-size: 22px;\"> 16. Now that we have identified the features for \"Good phase\", using an aggr method we can go ahead combine those features. </br><li>once features are combined as \"Good Phase\", we drop all those redundant features of months _6&_7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lst, new_cols = combine_features(df=telecom_df, cols=cols_to_combine, pat1='_6', pat2='_7',to_append='_good_phase')\n",
    "telecom_df = telecom_df.drop(drop_lst, axis=1)\n",
    "telecom_df.shape\n",
    "telecom_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_null,_ = check_col_null_pct(telecom_df)\n",
    "cols_with_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen; \"> Observation 8:\n",
    "The total no of columns are now reduced to 91 and all the null values are imputed</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df = telecom_df.drop('mobile_number', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = telecom_df.select_dtypes(include=np.number).columns\n",
    "fig, axs = plt.subplots(int(np.ceil(len(cols)/10)),10, figsize=(30, int(np.ceil(len(cols)/10))*2))\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    t1 = axs.flatten()[idx]\n",
    "    t1.hist(telecom_df[col])\n",
    "    t1.set_title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = telecom_df[telecom_df.select_dtypes(include=np.number).columns].corr()\n",
    "mask = np.zeros_like(corr_df)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "fig, axs = plt.subplots(figsize=(15,10))\n",
    "with sns.axes_style('white'):\n",
    "    axs = sns.heatmap(corr_df, mask=mask, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = telecom_df.corr(numeric_only=True).abs()\n",
    "corr_df = corr_df.unstack()\n",
    "correlation = corr_df.sort_values()\n",
    "correlation = corr_df.dropna()\n",
    "\n",
    "correlation = correlation [correlation  != 1.0]\n",
    "correlation = correlation .reset_index()\n",
    "correlation.sort_values(by=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# telecom_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = telecom_df.drop('churn', axis=1)\n",
    "y = telecom_df['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, stratify=y, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train[X_train.columns] = scaler.fit_transform(X_train, y_train)\n",
    "X_train\n",
    "\n",
    "X_test[X_test.columns] = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# telecom_df.to_csv('test.csv')\n",
    "telecom_df['churn'].sum()/len(telecom_df['churn'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ori, y_train_ori = X_train.copy(), y_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Function for Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(X_tr, y_tr, technique='oversampling', random_state=100):\n",
    "    if technique == 'undersampling':\n",
    "        under_sample = RandomUnderSampler(random_state=random_state, sampling_strategy='majority')\n",
    "        # print(under_sample.get_params())\n",
    "        # print(under_sample._sampling_strategy_docstring)\n",
    "        X_train_udr, y_train_udr = under_sample.fit_resample(X_tr, y_tr)\n",
    "        return X_train_udr, y_train_udr\n",
    "    elif technique == 'tomek_links':\n",
    "        tomek_sample = TomekLinks()\n",
    "        # print(tomek_sample.get_params())\n",
    "        # print(tomek_sample._sampling_strategy_docstring)\n",
    "        X_train_tomek, y_train_tomek = tomek_sample.fit_resample(X_tr, y_tr)\n",
    "        return X_train_tomek, y_train_tomek\n",
    "    elif technique == 'oversampling':\n",
    "        over_sample = RandomOverSampler(random_state=random_state)\n",
    "        # print(over_sample.get_params())\n",
    "        # print(over_sample._sampling_strategy_docstring)\n",
    "        X_train_ovr, y_train_ovr = over_sample.fit_resample(X_tr, y_tr)\n",
    "        return X_train_ovr, y_train_ovr\n",
    "    elif technique == 'smote':\n",
    "        smote_sample = SMOTE(random_state=random_state, k_neighbors=5)\n",
    "        # print(smote_sample.get_params())\n",
    "        # print(smote_sample._sampling_strategy_docstring)\n",
    "        X_train_smote, y_train_smote = smote_sample.fit_resample(X_tr, y_tr)\n",
    "        return X_train_smote, y_train_smote\n",
    "    elif technique == 'adasyn':\n",
    "        adasyn_sample = ADASYN(random_state=random_state, n_neighbors=5)\n",
    "        # print(adasyn_sample.get_params())\n",
    "        # print(adasyn_sample._sampling_strategy_docstring)\n",
    "        X_train_adasyn, y_train_adasyn = adasyn_sample.fit_resample(X_tr, y_tr)\n",
    "        return X_train_adasyn, y_train_adasyn\n",
    "    elif technique == 'smote_tomek':\n",
    "        smote_tomek_sample = SMOTETomek(random_state=random_state)\n",
    "        # print(smote_tomek_sample.get_params())\n",
    "        X_train_smote_tomek, y_train_smote_tomek = smote_tomek_sample.fit_resample(X_tr, y_tr)\n",
    "        return X_train_smote_tomek, y_train_smote_tomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class Imbalance using Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ovr, y_train_ovr = handle_imbalance(X_train_ori, y_train_ori, technique='oversampling', random_state=100)\n",
    "sum(y_train_ovr)/len(y_train_ovr)*100\n",
    "sorted(Counter(y_train_ovr).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class Imbalance using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote, y_train_smote = handle_imbalance(X_train_ori, y_train_ori, technique='smote', random_state=100)\n",
    "sum(y_train_smote)/len(y_train_smote)*100\n",
    "sorted(Counter(y_train_smote).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class Imbalance using ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adasyn, y_train_adasyn = handle_imbalance(X_train_ori, y_train_ori, technique='adasyn', random_state=100)\n",
    "sum(y_train_adasyn)/len(y_train_adasyn)*100\n",
    "sorted(Counter(y_train_adasyn).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component_analysis(X_tr, random_state=100):\n",
    "    pca = PCA(random_state=random_state)\n",
    "    pca.fit(X_tr)\n",
    "\n",
    "    var_ratio_df = pd.DataFrame({'feature':X_tr.columns, 'var_ratio': pca.explained_variance_ratio_})\n",
    "    components = pd.DataFrame(pca.components_)\n",
    "    var_ratio_df = pd.concat([var_ratio_df,components],axis=1)\n",
    "    var_ratio_df.head()\n",
    "    \n",
    "    fig, axs = plt.subplots(1,3, figsize=(25,5))\n",
    "    axs[0].bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "    axs[0].set_xlabel('Component number')\n",
    "    axs[0].set_ylabel('Explained variance ratio')\n",
    "    axs[0].set_title('Bar plot')\n",
    "\n",
    "    axs[1].plot(pca.explained_variance_ratio_)\n",
    "    axs[1].set_xlabel('Component number')\n",
    "    axs[1].set_ylabel('Explained variance ratio')\n",
    "    axs[1].set_title('Scree plot')\n",
    "\n",
    "    var_cumu = np.cumsum(pca.explained_variance_ratio_) \n",
    "    axs[2].vlines(x=47, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "    axs[2].hlines(y=0.95, xmax=100, xmin=0, colors=\"g\", linestyles=\"--\")\n",
    "    axs[2].plot(var_cumu)\n",
    "    axs[2].set_ylabel(\"Cumulative variance explained\")\n",
    "    axs[2].set_xlabel('Component number')\n",
    "    plt.show();\n",
    "    \n",
    "def incremental_pca(X_tr, X_te, n_components=45):\n",
    "    pca_incremental = IncrementalPCA(n_components, )\n",
    "    pca_incremental.fit(X_tr)\n",
    "\n",
    "    # Retrieve the principal components\n",
    "    principal_components = pca_incremental.components_\n",
    "    feature_names = X_tr.columns\n",
    "\n",
    "    # Calculate the overall importance of each feature\n",
    "    feature_importance = {}\n",
    "    for i, component in enumerate(principal_components):\n",
    "        for j, weight in enumerate(component):\n",
    "            if feature_names[j] not in feature_importance:\n",
    "                feature_importance[feature_names[j]] = 0.0\n",
    "            feature_importance[feature_names[j]] += abs(weight)\n",
    "\n",
    "    # Sort features by their overall importance\n",
    "    sorted_feature_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    feature_lst = []\n",
    "    for feature, importance in sorted_feature_importance[:10]:\n",
    "        feature_lst.append(f\"{feature}: {importance}\")\n",
    "\n",
    "    df_tr_pca = pca_incremental.fit_transform(X_tr)\n",
    "    df_te_pca = pca_incremental.transform(X_te)\n",
    "    \n",
    "    return df_tr_pca, df_te_pca, feature_names, feature_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA after using class imbalance techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA using Random Oversampling\")\n",
    "X_train , y_train = X_train_ovr, y_train_ovr \n",
    "principal_component_analysis(X_train, random_state=100)\n",
    "df_train_ovr_pca, df_test_ovr_pca, feature_names, feature_lst = incremental_pca(X_train, X_test, n_components=45)\n",
    "\n",
    "print(\"PCA using SMOTE\")\n",
    "X_train , y_train = X_train_smote, y_train_smote \n",
    "principal_component_analysis(X_train, random_state=100)\n",
    "df_train_smote_pca, df_test_smote_pca, feature_names, feature_lst = incremental_pca(X_train, X_test, n_components=45)\n",
    "\n",
    "print(\"PCA using ADASYN\")\n",
    "X_train , y_train = X_train_adasyn, y_train_adasyn \n",
    "principal_component_analysis(X_train, random_state=100)\n",
    "df_train_adasyn_pca, df_test_adasyn_pca, feature_names, feature_lst = incremental_pca(X_train, X_test, n_components=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning - Algorithms and Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(fXt, fyt, fcutoff, ftest=False, fres=None):\n",
    "    fXt_sm = sm.add_constant(fXt)\n",
    "    \n",
    "    if ftest == False:\n",
    "        lrm = sm.GLM(fyt, fXt_sm, family=sm.families.Binomial())\n",
    "        lrm = lrm.fit()\n",
    "        fyt_pred = lrm.predict(fXt_sm)\n",
    "    else:\n",
    "        lrm = fres\n",
    "        fyt_pred = lrm.predict(fXt_sm)\n",
    "        \n",
    "    fyt_pred = fyt_pred.values.reshape(-1)\n",
    "    fyt_pred_final = pd.DataFrame({'Converted': fyt.values, 'Conv_Prob': fyt_pred})\n",
    "    fyt_pred_final['ID'] = fyt.index\n",
    "    fyt_pred_final['predicted'] = fyt_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return lrm, fyt_pred, fyt_pred_final\n",
    "\n",
    "def logreg_metrics_fn(fyt_pred_final):\n",
    "    fconfusion = confusion_matrix(fyt_pred_final.Converted, fyt_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fyt_pred_final.Converted, fyt_pred_final.predicted)\n",
    "    \n",
    "    TP = fconfusion[1,1] # true positive \n",
    "    TN = fconfusion[0,0] # true negatives\n",
    "    FP = fconfusion[0,1] # false positives\n",
    "    FN = fconfusion[1,0] # false negatives\n",
    "    \n",
    "    fSensi  = TP/(TP+FN) # Calculate the sensitivity\n",
    "    fSpeci  = TN/(TN+FP) # Calculate the specificity\n",
    "    fPreci  = TP/(TP+FP) # Calculate Precision\n",
    "    fRecal  = TP/(TP+FN) # Calculate Recall\n",
    "       \n",
    "    return fconfusion, faccuracy, fSensi, fSpeci, fPreci, fRecal\n",
    "\n",
    "def get_vif_score(fXt, cl):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['Features'] = fXt[cl].columns\n",
    "    vif['VIF'] = [variance_inflation_factor(fXt[cl].values, i) for i in range(fXt[cl].shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return vif\n",
    "\n",
    "def generate_metrics(yt, yt_pred, yt_prob ):\n",
    "    accuracy = accuracy_score(yt, yt_pred,  )\n",
    "    f1_sc = f1_score(yt, yt_pred,  )\n",
    "    recall = recall_score(yt, yt_pred,  )\n",
    "    precision = precision_score(yt, yt_pred,  )\n",
    "    roc_score = roc_auc_score(yt, yt_prob)\n",
    "    cl_rep = classification_report(yt, yt_pred)\n",
    "    conf_mat = confusion_matrix(yt, yt_pred)\n",
    "    df = pd.DataFrame({\"accuracy\":[accuracy],\"roc_score\":[roc_score],\"precision\":[precision],\"recall\":[recall],\"f1_score\":[f1_sc], \n",
    "                       \"classification_report\":[cl_rep], \"confusion_matrix\": [conf_mat]})\n",
    "    return df, accuracy,roc_score, precision,recall, f1_sc, cl_rep, conf_mat\n",
    "\n",
    "def generate_summary_report(df=None, model_name=\"\", class_imb='', train_accuracy=\"\", test_accuracy=\"\", roc_score=\"\", precision=\"\", recall=\"\", f1_score=\"\", classification_rep=\"\", conf_matrix=\"\", step='create'):\n",
    "    if step == 'create':\n",
    "        df = pd.DataFrame(columns=[\"model_name\",\"class_imb\",\"train_accuracy\",\"test_accuracy\",\"roc_score\",\"precision\",\"recall\",\"f1_score\", \"classification_report\", \"confusion_matrix\"])\n",
    "        return df\n",
    "    elif step == 'add': \n",
    "        df.loc[len(df)] = pd.Series({\"model_name\": model_name,\"class_imb\":class_imb,\"train_accuracy\": train_accuracy, \"test_accuracy\": test_accuracy,\"roc_score\": roc_score,\"precision\": precision,\"recall\": recall,\"f1_score\":f1_score, \"classification_report\": classification_rep, \"confusion_matrix\": conf_matrix})\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg = logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(estimator=logreg, cv=5)\n",
    "rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.cv_results_['mean_test_score']\n",
    "plt.figure(figsize=[10, 5])\n",
    "plt.plot(range(1, len(X_train.columns)+1), rfecv.cv_results_['mean_test_score'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=logreg, n_features_to_select= 15) \n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "cols = rfe.get_feature_names_out()\n",
    "rfe_df = pd.DataFrame({'feature':X_train.columns, 'rank': rfe.ranking_ , 'support': rfe.support_})\n",
    "rfe_df.sort_values(by='rank', ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sm.GLM(y_train, sm.add_constant(X_train), family=sm.families.Binomial())\n",
    "logreg = logreg.fit()\n",
    "# logreg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg1, y_train_pred, y_train_pred_final = model_training(X_train[cols], y_train, 0.5)\n",
    "logreg1.pvalues.sort_values(ascending=False).head()\n",
    "\n",
    "vif_scores = get_vif_score(X_train, cols)\n",
    "vif_scores[vif_scores['VIF'] > 5].head()\n",
    "\n",
    "# cf_matrix, accuracy, sensitivity, specificity, precision, recall = logreg_metrics_fn(y_train_pred_final)\n",
    "# print(f'Sensitivity - {round(sensitivity,3)}\\nspecificity - {round(specificity,3)}\\nPrecision - {round(precision,3)}\\nRecall - {round(recall,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(cols)\n",
    "cols.remove('total_ic_mou_good_phase')\n",
    "\n",
    "logreg2, y_train_pred, y_train_pred_final = model_training(X_train[cols], y_train, 0.5)\n",
    "logreg2.pvalues.sort_values(ascending=False).head()\n",
    "\n",
    "vif_scores = get_vif_score(X_train,cols)\n",
    "vif_scores[vif_scores['VIF'] > 5]\n",
    "\n",
    "# cf_matrix, accuracy, sensitivity, specificity, precision, recall = logreg_metrics_fn(y_train_pred_final)\n",
    "# print(f'Sensitivity - {round(sensitivity,3)}\\nspecificity - {round(specificity,3)}\\nPrecision - {round(precision,3)}\\nRecall - {round(recall,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('total_ic_mou_action_phase')\n",
    "\n",
    "logreg3, y_train_pred, y_train_pred_final = model_training(X_train[cols], y_train, 0.5)\n",
    "logreg3.pvalues.sort_values(ascending=False).head()\n",
    "\n",
    "vif_scores = get_vif_score(X_train,cols)\n",
    "vif_scores[vif_scores['VIF'] > 5]\n",
    "\n",
    "# cf_matrix, accuracy, sensitivity, specificity, precision, recall = logreg_metrics_fn(y_train_pred_final)\n",
    "# print(f'Sensitivity - {round(sensitivity,3)}\\nspecificity - {round(specificity,3)}\\nPrecision - {round(precision,3)}\\nRecall - {round(recall,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('total_og_mou_good_phase')\n",
    "\n",
    "logreg4, y_train_pred, y_train_pred_final = model_training(X_train[cols], y_train, 0.5)\n",
    "logreg4.pvalues.sort_values(ascending=False).head()\n",
    "\n",
    "vif_scores = get_vif_score(X_train,cols)\n",
    "vif_scores[vif_scores['VIF'] > 5].head()\n",
    "\n",
    "# cf_matrix, accuracy, sensitivity, specificity, precision, recall = logreg_metrics_fn(y_train_pred_final)\n",
    "# print(f'Sensitivity - {round(sensitivity,3)}\\nspecificity - {round(specificity,3)}\\nPrecision - {round(precision,3)}\\nRecall - {round(recall,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg5, y_train_pred, y_train_pred_final = model_training(X_train[cols], y_train, 0.5)\n",
    "logreg5.pvalues.sort_values(ascending=False).head()\n",
    "\n",
    "vif_scores = get_vif_score(X_train,cols)\n",
    "vif_scores.head()\n",
    "\n",
    "# cf_matrix, accuracy, sensitivity, specificity, precision, recall = logreg_metrics_fn(y_train_pred_final)\n",
    "# print(f'Sensitivity - {round(sensitivity,3)}\\nspecificity - {round(specificity,3)}\\nPrecision - {round(precision,3)}\\nRecall - {round(recall,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Conv_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head(2)\n",
    "\n",
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci', 'preci', 'recall'])\n",
    "for i in numbers:\n",
    "    cm1 = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    preci = cm1[1,1]/(cm1[0,1]+cm1[1,1])\n",
    "    recall = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[i, accuracy, sensi, speci, preci, recall]\n",
    "    \n",
    "cutoff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(25,6))\n",
    "\n",
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'], ax=axs[0])\n",
    "axs[0].set_title('Accuracy Sensitivity Specificity')\n",
    "axs[0].vlines( ymin=0, ymax=0.9,x=0.537, color=\"r\")\n",
    "\n",
    "# created a function for the ROC curve creation and see the labels \n",
    "RocCurveDisplay.from_predictions(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob, drop_intermediate=False, ax=axs[1 ])\n",
    "axs[1].set_title('ROC curve')\n",
    "\n",
    "# plotting Precision and Recall curve and finding the cutoff for this.\n",
    "p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)\n",
    "axs[2].plot(thresholds, p[:-1], \"b\")\n",
    "axs[2].plot(thresholds, r[:-1], \"r\")\n",
    "axs[2].set_title('Precision Recall Curve')\n",
    "axs[2].vlines( ymin=0, ymax=0.9,x=0.54, color=\"r\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used the cutoff value from accuracy sensitivity and specificity curve to see the prediction on train dataset.\n",
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.537 else 0)\n",
    "y_train_pred_final.head()\n",
    "\n",
    "# Let's check the overall accuracy.\n",
    "cf_matrix, train_accuracy, sensitivity, specificity, precision, recall = logreg_metrics_fn(y_train_pred_final)\n",
    "print(f'Train Accuracy - {round(train_accuracy,3)}\\nSensitivity - {round(sensitivity,3)}\\nspecificity - {round(specificity,3)}\\nPrecision - {round(precision,3)}\\nRecall - {round(recall,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used the cutoff value from precision -recall curve to see the prediction on train dataset.\n",
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.54 else 0)\n",
    "y_train_pred_final.head()\n",
    "\n",
    "# Let's check the overall accuracy. \n",
    "cf_matrix, train_accuracy,sensitivity, specificity, precision, recall = logreg_metrics_fn(y_train_pred_final)\n",
    "print(f'Train Accuracy - {round(train_accuracy,3)}\\nSensitivity - {round(sensitivity,3)}\\nspecificity - {round(specificity,3)}\\nPrecision - {round(precision,3)}\\nRecall - {round(recall,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregp, y_test_pred, y_test_pred_final = model_training(X_test[cols], y_test, 0.54, True, logreg5)\n",
    "\n",
    "cf_matrix, test_accuracy, sensitivity, specificity, precision, recall = logreg_metrics_fn(y_test_pred_final)\n",
    "roc_score = roc_auc_score( y_test_pred_final.Converted, y_test_pred_final.Conv_Prob )\n",
    "f1_sc = f1_score(y_test, y_test_pred_final.predicted)\n",
    "\n",
    "print(f'Test Accuracy - {round(test_accuracy,3)}\\nROC Score - {round(roc_score,3)}\\nSensitivity - {round(sensitivity,3)}\\nSpecificity - {round(specificity,3)}\\nPrecision - {round(precision,3)}\\nRecall - {round(recall,3)}')\n",
    "\n",
    "overall_summary_df = generate_summary_report()\n",
    "overall_summary_df = generate_summary_report(df=overall_summary_df, model_name=\"LogisticRegression\",class_imb='oversampling', train_accuracy=train_accuracy, test_accuracy=test_accuracy, roc_score=roc_score, \n",
    "                                             precision=precision, recall=recall, f1_score=f1_sc, classification_rep=\"-\", conf_matrix=cf_matrix, step='add')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LogisticRegressionClassifier\n",
    "def logistic_regression(df_train_pca, df_test_pca, y_train, y_test, overall_summary_df):\n",
    "    lr_pca = LogisticRegression()\n",
    "    lr_pca.fit(df_train_pca, y_train)\n",
    "\n",
    "    lr_pca_ytrain_prob = lr_pca.predict_proba(df_train_pca)[:,1]\n",
    "    lr_pca_ytrain_pred = lr_pca.predict(df_train_pca)\n",
    "\n",
    "    lr_pca_ytest_prob = lr_pca.predict_proba(df_test_pca)[:,1]\n",
    "    lr_pca_ytest_pred = lr_pca.predict(df_test_pca)\n",
    "\n",
    "    _, train_accuracy, _, _, _, _, _, _ = generate_metrics(y_train, yt_pred=lr_pca_ytrain_pred, yt_prob=lr_pca_ytrain_prob)\n",
    "    metrics_df, test_accuracy, roc_score, precision,recall, f1_sc, class_report, conf_matrix = generate_metrics(y_test, yt_pred=lr_pca_ytest_pred, yt_prob=lr_pca_ytest_prob)\n",
    "    print(metrics_df)\n",
    "\n",
    "    overall_summary_df = generate_summary_report(df=overall_summary_df, model_name=\"LogisticRegressionPCA\",class_imb='oversampling', train_accuracy=train_accuracy, test_accuracy=test_accuracy, roc_score=roc_score, \n",
    "                                                precision=precision, recall=recall, f1_score=f1_sc, classification_rep=class_report, conf_matrix=conf_matrix, step='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr \n",
    "logistic_regression(df_train_ovr_pca, df_test_ovr_pca, y_train, y_test, overall_summary_df = overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees - Base \n",
    "def decision_tree_classifier(X_train, y_train, X_test, y_test, technique, overall_summary_df):\n",
    "    dtc = DecisionTreeClassifier(random_state=100)\n",
    "    dtc = dtc.fit(X_train, y_train)\n",
    "\n",
    "    # Get feature importance\n",
    "    dtc_feature_importances = dtc.feature_importances_\n",
    "    if X_train.shape[1] == len(X.columns):\n",
    "        model_name=\"DecisionTreeClassifier\"\n",
    "        dtc_feature_imp_df = pd.DataFrame({'columns':X.columns, 'imp_score': dtc_feature_importances})\n",
    "    else:\n",
    "        model_name=\"DecisionTreeClassifierPCA\"\n",
    "        dtc_feature_imp_df = pd.DataFrame({'principal_component': range(len(dtc_feature_importances)), 'imp_score': dtc_feature_importances})\n",
    "    dtc_feature_imp_df.sort_values(by='imp_score', ascending=False).head(5)\n",
    "\n",
    "    y_train_prob_dtc = dtc.predict_proba(X_train)[:, 1]\n",
    "    y_train_pred_dtc = dtc.predict(X_train)\n",
    "\n",
    "    y_test_prob_dtc = dtc.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred_dtc = dtc.predict(X_test)\n",
    "\n",
    "    _, train_accuracy, _, _, _, _, _, _ = generate_metrics(y_train, yt_pred=y_train_pred_dtc, yt_prob=y_train_prob_dtc)\n",
    "    metrics_df, test_accuracy, roc_score, precision,recall, f1_sc, class_report, conf_matrix = generate_metrics(y_test, yt_pred=y_test_pred_dtc, yt_prob=y_test_prob_dtc)\n",
    "    print(metrics_df)\n",
    "    \n",
    "    overall_summary_df = generate_summary_report(df=overall_summary_df, model_name=model_name, class_imb=technique, train_accuracy=train_accuracy, test_accuracy=test_accuracy, roc_score=roc_score, \n",
    "                                                precision=precision, recall=recall, f1_score=f1_sc, classification_rep =class_report, conf_matrix=conf_matrix, step='add')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DT - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr\n",
    "decision_tree_classifier(X_train, y_train, X_test, y_test,  technique='oversampling', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DT - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "decision_tree_classifier(X_train, y_train, X_test, y_test, technique='smote', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DT - ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_adasyn, y_train_adasyn\n",
    "decision_tree_classifier(X_train, y_train, X_test, y_test, technique='adasyn', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr \n",
    "decision_tree_classifier(df_train_ovr_pca, y_train, df_test_ovr_pca, y_test, technique='oversampling', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(X_train, y_train, X_test, y_test, technique, overall_summary_df):\n",
    "    rf = RandomForestClassifier(random_state=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Get feature importance\n",
    "    rf_feature_importances = rf.feature_importances_\n",
    "    if X_train.shape[1] == len(X.columns):\n",
    "        model_name=\"RandomForestClassifier\"\n",
    "        rf_feature_imp_df = pd.DataFrame({'columns':X.columns, 'imp_score': rf_feature_importances})\n",
    "    else:\n",
    "        model_name=\"RandomForestClassifierPCA\"\n",
    "        rf_feature_imp_df = pd.DataFrame({'principal_component': range(len(rf_feature_importances)), 'imp_score': rf_feature_importances})\n",
    "    rf_feature_imp_df.sort_values(by='imp_score', ascending=False).head(10)\n",
    "\n",
    "    rf_ytrain_prob = rf.predict_proba(X_train)[:, 1]\n",
    "    rf_ytrain_pred = rf.predict(X_train)\n",
    "\n",
    "    rf_ytest_prob = rf.predict_proba(X_test)[:, 1]\n",
    "    rf_ytest_pred = rf.predict(X_test)\n",
    "\n",
    "    _, train_accuracy, _, _, _, _, _, _ = generate_metrics(y_train, yt_pred=rf_ytrain_pred, yt_prob=rf_ytrain_prob)\n",
    "    metrics_df, test_accuracy, roc_score, precision,recall, f1_sc, class_report, conf_matrix = generate_metrics(y_test, yt_pred=rf_ytest_pred, yt_prob=rf_ytest_prob)\n",
    "    print(metrics_df)\n",
    "\n",
    "    overall_summary_df = generate_summary_report(df=overall_summary_df, model_name=model_name,class_imb=technique, train_accuracy=train_accuracy, test_accuracy=test_accuracy, roc_score=roc_score, \n",
    "                                                precision=precision, recall=recall, f1_score=f1_sc, classification_rep=class_report, conf_matrix=conf_matrix, step='add')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr\n",
    "random_forest_classifier(X_train, y_train, X_test, y_test,  technique='oversampling', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "random_forest_classifier(X_train, y_train, X_test, y_test, technique='smote', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest - ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_adasyn, y_train_adasyn\n",
    "random_forest_classifier(X_train, y_train, X_test, y_test, technique='adasyn', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr \n",
    "random_forest_classifier(df_train_ovr_pca, y_train, df_test_ovr_pca, y_test, technique='oversampling', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_classifier(X_train, y_train, X_test, y_test, technique, overall_summary_df):\n",
    "    gbc = GradientBoostingClassifier(random_state=100)\n",
    "    gbc.fit(X_train, y_train)\n",
    "\n",
    "    # Get feature importance\n",
    "    gbc_feature_importances = gbc.feature_importances_\n",
    "    if X_train.shape[1] == len(X.columns):\n",
    "        model_name=\"GradientBoostingClassifier\"\n",
    "        gbc_feature_imp_df = pd.DataFrame({'columns':X.columns, 'imp_score': gbc_feature_importances})\n",
    "    else:\n",
    "        model_name=\"GradientBoostingClassifierPCA\"\n",
    "        gbc_feature_imp_df = pd.DataFrame({'principal_component': range(len(gbc_feature_importances)), 'imp_score': gbc_feature_importances})    \n",
    "    gbc_feature_imp_df.sort_values(by='imp_score', ascending=False).head(10)\n",
    "\n",
    "    gbc_ytrain_prob = gbc.predict_proba(X_train)[:, 1]\n",
    "    gbc_ytrain_pred = gbc.predict(X_train)\n",
    "\n",
    "    gbc_ytest_prob = gbc.predict_proba(X_test)[:, 1]\n",
    "    gbc_ytest_pred = gbc.predict(X_test)\n",
    "\n",
    "    _, train_accuracy, _, _, _, _, _, _ = generate_metrics(y_train, yt_pred=gbc_ytrain_pred, yt_prob=gbc_ytrain_prob)\n",
    "    metrics_df, test_accuracy, roc_score, precision,recall, f1_sc, class_report, conf_matrix = generate_metrics(y_test, yt_pred=gbc_ytest_pred, yt_prob=gbc_ytest_prob)\n",
    "    print(metrics_df)\n",
    "\n",
    "    overall_summary_df = generate_summary_report(df=overall_summary_df, model_name=model_name, class_imb=technique, train_accuracy=train_accuracy, test_accuracy=test_accuracy, roc_score=roc_score, \n",
    "                                                precision=precision, recall=recall, f1_score=f1_sc, classification_rep=class_report, conf_matrix=conf_matrix, step='add')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr\n",
    "gradient_boosting_classifier(X_train, y_train, X_test, y_test,  technique='oversampling', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "gradient_boosting_classifier(X_train, y_train, X_test, y_test, technique='smote', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting - ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_adasyn, y_train_adasyn\n",
    "gradient_boosting_classifier(X_train, y_train, X_test, y_test, technique='adasyn', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr \n",
    "gradient_boosting_classifier(df_train_ovr_pca, y_train, df_test_ovr_pca, y_test, technique='oversampling', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_classifier(X_train, y_train, X_test, y_test, technique, overall_summary_df):\n",
    "    xgb_cfl =  xgb.XGBClassifier(scale_pos_weight= 1, objective = 'binary:logistic', random_state= 100)\n",
    "    xgb_cfl.fit(X_train, y_train)\n",
    "\n",
    "    # Get feature importance\n",
    "    xgb_feature_importances = xgb_cfl.feature_importances_\n",
    "    if X_train.shape[1] == len(X.columns):\n",
    "        model_name=\"XGBClassifier\"\n",
    "        xgb_feature_imp_df = pd.DataFrame({'columns':X.columns, 'imp_score': xgb_feature_importances})\n",
    "    else:\n",
    "        model_name=\"XGBClassifier\"\n",
    "        xgb_feature_imp_df = pd.DataFrame({'principal_component': range(len(xgb_feature_importances)), 'imp_score': xgb_feature_importances}) \n",
    "    xgb_feature_imp_df.sort_values(by='imp_score', ascending=False).head(10)\n",
    "\n",
    "    xgb_ytrain_prob = xgb_cfl.predict_proba(X_train)[:, 1]\n",
    "    xgb_ytrain_pred = xgb_cfl.predict(X_train)\n",
    "\n",
    "    xgb_ytest_prob = xgb_cfl.predict_proba(X_test)[:, 1]\n",
    "    xgb_ytest_pred = xgb_cfl.predict(X_test)\n",
    "\n",
    "    _, train_accuracy, _, _, _, _, _, _ = generate_metrics(y_train, yt_pred=xgb_ytrain_pred, yt_prob=xgb_ytrain_prob)\n",
    "    metrics_df, test_accuracy, roc_score, precision,recall, f1_sc, class_report, conf_matrix = generate_metrics(y_test, yt_pred=xgb_ytest_pred, yt_prob=xgb_ytest_prob)\n",
    "    print(metrics_df)\n",
    "\n",
    "    overall_summary_df = generate_summary_report(df=overall_summary_df, model_name=model_name, class_imb=technique, train_accuracy=train_accuracy, test_accuracy=test_accuracy, roc_score=roc_score, \n",
    "                                                precision=precision, recall=recall, f1_score=f1_sc, classification_rep=class_report, conf_matrix=conf_matrix, step='add') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr\n",
    "xgb_classifier(X_train, y_train, X_test, y_test,  technique='oversampling', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_smote, y_train_smote\n",
    "xgb_classifier(X_train, y_train, X_test, y_test, technique='smote', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost - ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_adasyn, y_train_adasyn\n",
    "xgb_classifier(X_train, y_train, X_test, y_test, technique='adasyn', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_ovr, y_train_ovr \n",
    "xgb_classifier(df_train_ovr_pca, y_train, df_test_ovr_pca, y_test, technique='oversampling', overall_summary_df=overall_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_summary_df.set_index('model_name').plot.bar(figsize=(30,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsbasic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
